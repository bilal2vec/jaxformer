{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import jax\n",
    "from jax.experimental.maps import xmap\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.custom_vjp\n",
    "def f_psum(x):\n",
    "    return x\n",
    "\n",
    "def f_psum_fwd(x):\n",
    "    return f_psum(x), None\n",
    "\n",
    "def f_psum_bwd(_, g):\n",
    "    return jax.lax.psum(g, \"shard\"),\n",
    "\n",
    "f_psum.defvjp(f_psum_fwd, f_psum_bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.custom_vjp\n",
    "def g_psum(x):\n",
    "    return jax.lax.psum(x, \"shard\")\n",
    "\n",
    "def g_psum_fwd(x):\n",
    "    return g_psum(x), None\n",
    "\n",
    "def g_psum_bwd(_, g):\n",
    "    return g,\n",
    "\n",
    "g_psum.defvjp(g_psum_fwd, g_psum_bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard(xs):\n",
    "    return jax.tree_map(\n",
    "        lambda x: x.reshape((jax.device_count(), -1) + x.shape[1:]), xs)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    config: dataclass\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "\n",
    "        d_model_mp = d_model // config.mp\n",
    "        d_k = d_model_mp // config.n_heads\n",
    "        \n",
    "        x = f_psum(x)\n",
    "\n",
    "        q = nn.Dense(d_model_mp)(x)\n",
    "        k = nn.Dense(d_model_mp)(x)\n",
    "        v = nn.Dense(d_model_mp)(x)\n",
    "\n",
    "        # batch x n_heads x seq_len x d_k\n",
    "        q = q.reshape((-1, seq_len, config.n_heads, d_k)).transpose((0, 2, 1, 3))\n",
    "        k = k.reshape((-1, seq_len, config.n_heads, d_k)).transpose((0, 2, 1, 3))\n",
    "        v = v.reshape((-1, seq_len, config.n_heads, d_k)).transpose((0, 2, 1, 3))\n",
    "\n",
    "        a = jnp.matmul(q, k.transpose((0, 1, 3, 2))) / jnp.sqrt(d_k)\n",
    "\n",
    "        mask = jnp.where(mask, 0, -jnp.inf)\n",
    "        a += mask\n",
    "\n",
    "        a = nn.softmax(a, axis=-1)\n",
    "        a = jnp.matmul(a, v)\n",
    "\n",
    "        # batch x seq_len x n_heads x d_k\n",
    "        # batch x seq_len x d_model_mp\n",
    "        a = a.transpose((0, 2, 1, 3)).reshape(-1, seq_len, d_model_mp)\n",
    "\n",
    "        o = nn.Dense(d_model)(a)\n",
    "        o = g_psum(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    config: dataclass\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = f_psum(x)\n",
    "\n",
    "        x = nn.Dense((self.config.d_model // self.config.mp) * 4)(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dense(self.config.d_model)(x)\n",
    "\n",
    "        x = g_psum(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    config: dataclass\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask):\n",
    "\n",
    "        x = x + MultiHeadSelfAttention(self.config)(nn.LayerNorm()(x), mask)\n",
    "        x = x + MLP(self.config)(nn.LayerNorm()(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    config: dataclass\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        seq_len = x.shape[-1]\n",
    "\n",
    "        position_ids = jnp.arange(start=0, stop=seq_len, step=1)\n",
    "        mask = jnp.triu(\n",
    "            jnp.ones((1, seq_len, seq_len)), k=1) == 0\n",
    "\n",
    "        content_embedding = nn.Embed(self.config.vocab_size, self.config.d_model)\n",
    "        x = content_embedding(x) + nn.Embed(self.config.max_seq_len, self.config.d_model)(position_ids)\n",
    "\n",
    "        for _ in range(self.config.n_layers):\n",
    "            x = Block(config)(x, mask)\n",
    "\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = content_embedding.attend(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    fast: bool = False\n",
    "\n",
    "    mp: int = 3\n",
    "    dp: int = 2\n",
    "\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 30\n",
    "    sample_len: int = 16\n",
    "\n",
    "    max_seq_len: int = 128\n",
    "    n_layers: int = 2\n",
    "    vocab_size: int = 32768\n",
    "    d_model: int = 768\n",
    "    n_heads: int = 8\n",
    "config = Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file('./tokenizer.json')\n",
    "with open('./wikitext-2-raw/wiki.train.raw', 'r') as f:\n",
    "    text = f.read()\n",
    "tokenized = tokenizer.encode(text)\n",
    "batches = jnp.array(tokenized.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(xmap, in_axes=([\"shard\", ...], [\"batch\", ...]), out_axes=[\"shard\", ...])\n",
    "def initialize(rng, init_batch):\n",
    "    variables = GPT2(Config).init({'params': rng}, init_batch)\n",
    "    optimizer = flax.optim.Adam(learning_rate=1e-4, beta1=0.5, beta2=0.9).create(variables)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def loss_fn(variables, batch):\n",
    "    x = batch[:, :-1]\n",
    "    y = batch[:, 1:]\n",
    "    y = jax.nn.one_hot(y, config.vocab_size)\n",
    "\n",
    "    y_hat = GPT2(config).apply(variables, x)\n",
    "\n",
    "    loss = jnp.sum(y * jax.nn.log_softmax(y_hat, axis=-1), axis=-1)\n",
    "    return -jnp.mean(loss)\n",
    "\n",
    "@partial(xmap, in_axes=([\"shard\", ...], [\"batch\", ...]), out_axes=([\"shard\", ...], [\"batch\", ...]))\n",
    "def train_step(optimizer, batch):\n",
    "    loss, grad = jax.value_and_grad(loss_fn)(optimizer.target, batch)\n",
    "\n",
    "    loss = jax.lax.pmean(loss, axis_name=['shard'])\n",
    "    grad = jax.lax.pmean(grad, axis_name=['batch'])\n",
    "\n",
    "    optimizer = optimizer.apply_gradient(grad)\n",
    "\n",
    "    return optimizer, loss\n",
    "\n",
    "@partial(xmap, in_axes=([\"shard\", ...], [\"batch\", ...]), out_axes=([\"batch\", ...]))\n",
    "def eval_step(optimizer, batch):\n",
    "    loss = loss_fn(optimizer.target, batch)\n",
    "    loss = jax.lax.pmean(loss, axis_name='shard')\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "rngs = jax.random.split(rng, num=config.mp)\n",
    "init_batches = jnp.zeros((config.dp, 1, 128), dtype=int)\n",
    "\n",
    "optimizer = initialize(rngs, init_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = init_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, loss = train_step(optimizer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeviceArray([12.3784895, 12.3784895], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = eval_step(optimizer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeviceArray([1.674041, 1.674041], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}